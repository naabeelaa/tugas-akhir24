# -*- coding: utf-8 -*-
"""fixed ganjar mahfud multinomialnb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tPCzQY54QZrNmkr18yTdJB0KldLAIhkP
"""

import pandas as pd
import re

df = pd.read_excel('ganjar mahfud final.xlsx')
df.head()

"""#casefolding"""

df['casefolding'] = df['full_text'].str.lower()
df.head()

"""#cleaning data"""

def cleaning(text):
    text = re.sub(r'@', '', text)
    text = re.sub(r'#', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text.strip()

df['cleaning'] = df['casefolding'].apply(cleaning)

df[['casefolding', 'cleaning']].head()

"""# tokenisasi"""

import nltk
nltk.download('punkt_tab')

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

df['tokens'] = df['cleaning'].apply(word_tokenize)

df[['cleaning', 'tokens']].head()

df['tokens'] = df['cleaning'].apply(lambda x: x.split())

df[['cleaning', 'tokens']].head()

"""# stemming"""

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

df['stemmed_tokens'] = df['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])

df[['tokens', 'stemmed_tokens']].head()

"""#normalisasi"""

# List Manual
normalization_dict = {
    "jd": "jadi", "jgn": "jangan", "dgn": "dengan", "kira2": "kira kira", "spt": "seperti",
    "dlm": "dalam", "Mlm": "malam", "bgt": "banget", "udh": "udah", "utk": "untuk", "dah": "udah",
    "trs": "terus", "klo": "kalau", "emg": "emang", "mon": "mohon", "maap": "maaf", "gpp": "gapapa",
    "gmn": "gimana", "gegara": "gara gara", "ga": "nggak", "kaga": "tidak", "nga": "nggak",
    "ttg": "tentang", "pda": "pada", "skrng": "sekarang", "kyk": "kaya", "btw": "by the way",
    "yg": "yang", "dll": "dan lain lain", "emg": "emang", "utk": "untuk", "sm": "sama",
    "pd": "pada", "bkn": "bukan", "tp": "tapi", "tdk": "tidak", "sie": "sih", "mo": "mau",
    "kuatir": "khawatir", "kl": "kalau", "krn": "karena", "n": "dan", "ttp": "tetap", "bs": "bisa",
    "smg": "semoga", "lg": "lagi", "kel": "kelurahan", "kec": "kecamatan", "kab": "kabupaten",
    "prov": "provinsi", "bertg": "bertanggung", "jikalo": "jikalau", "ojo": "jangan", "belio": "beliau",
    "kp": "kampung", "klean": "kalian", "P.Diponegoro": "Pangeran Diponegoro", "klu": "kalau",
    "sy": "saya", "bgs": "bagus", "hrs": "harus", "sblm": "sebelum", "bnyk": "banyak", "cb": "coba",
    "sdh": "sudah", "br": "baru", "hrp": "harap", "bngt": "banget", "jend": "jenderal", "msh": "masih",
    "syny": "sayang", "dr": "dari", "jkw": "jokowi", "lg": "lagi", "Ig": "instagram", "kta": "kata",
    "sngking": "sangking", "aj": "aja", "orng": "orang", "nnti": "nanti", "wtk": "waktu",
    "gub": "gubernur", "ngmng": "ngomong", "ae": "aja", "makasih": "terimakasih", "ko": "kau",
    "gtu": "gitu", "sgtu": "segitu", "kdg": "kadang", "ane": "aku", "temlen": "timeline",
    "ultah": "ulang tahun", "dg": "dengan", "knp": "kenapa", "klah": "kalah", "mlwan": "melawan",
}

def normalize_tokens(tokens):
    return [normalization_dict.get(token, token) for token in tokens]

df['normalized_tokens'] = df['stemmed_tokens'].apply(normalize_tokens)

df[['stemmed_tokens', 'normalized_tokens']].head()

"""#stopword"""

!pip install Sastrawi

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Sastrawi
factory = StopWordRemoverFactory()
stopword_list = factory.get_stop_words()
stop_words = set(stopword_list)

def remove_stopwords(tokens):
    return [token for token in tokens if token not in stop_words]

df['filtered_tokens'] = df['normalized_tokens'].apply(remove_stopwords)

df[['normalized_tokens', 'filtered_tokens']].head()

df.to_excel('preprocessing.xlsx', index=False)

"""#TF-IDF"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

def tokens_to_string(tokens):
    return ' '.join(tokens)

def calculate_tfidf(df, token_column='filtered_tokens'):

    df['text_for_tfidf'] = df[token_column].apply(tokens_to_string)

    tfidf_vectorizer = TfidfVectorizer()

    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_for_tfidf'])

    feature_names = tfidf_vectorizer.get_feature_names_out()

    tfidf_df = pd.DataFrame(
        tfidf_matrix.toarray(),
        columns=feature_names
    )

    tfidf_df.index = df.index

    return tfidf_df, tfidf_vectorizer

def get_top_terms(tfidf_df, n_terms=10):
    """Get top terms by average TF-IDF score across all documents"""
    mean_tfidf = tfidf_df.mean()
    top_terms = mean_tfidf.sort_values(ascending=False).head(n_terms)
    return top_terms

def get_document_top_terms(tfidf_df, doc_index, n_terms=5):
    """Get top terms for a specific document"""
    doc_tfidf = tfidf_df.iloc[doc_index]
    top_terms = doc_tfidf.sort_values(ascending=False).head(n_terms)
    return top_terms

tfidf_df, vectorizer = calculate_tfidf(df)

top_terms = get_top_terms(tfidf_df, n_terms=10)
print("\nTop 10 terms across all documents:")
print(top_terms)

print("\nTop 5 terms in first document:")
print(get_document_top_terms(tfidf_df, 0))

tfidf_df.to_excel('tfidf_matrix.xlsx')

def create_summary_df(df, tfidf_df, n_terms=5):
    summaries = []
    for idx in range(len(df)):
        top_terms = get_document_top_terms(tfidf_df, idx, n_terms)
        summaries.append({
            'original_text': df['full_text'].iloc[idx],
            'top_terms': ', '.join(f"{term} ({score:.3f})" for term, score in top_terms.items())
        })
    return pd.DataFrame(summaries)

summary_df = create_summary_df(df, tfidf_df)
summary_df.to_excel('tfidf_summary.xlsx', index=False)

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

def tokens_to_string(tokens):
    return ' '.join(tokens)

def calculate_bag_of_words(df, token_column='filtered_tokens'):

    df['text_for_bow'] = df[token_column].apply(tokens_to_string)

    bow_vectorizer = CountVectorizer()

    bow_matrix = bow_vectorizer.fit_transform(df['text_for_bow'])

    feature_names = bow_vectorizer.get_feature_names_out()

    bow_df = pd.DataFrame(
        bow_matrix.toarray(),
        columns=feature_names
    )

    bow_df.index = df.index

    return bow_df, bow_vectorizer

def get_top_terms(bow_df, n_terms=10):

    total_counts = bow_df.sum()

    top_terms = total_counts.sort_values(ascending=False).head(n_terms)

    return top_terms

def get_document_top_terms(bow_df, doc_index, n_terms=5):
    doc_bow = bow_df.iloc[doc_index]
    top_terms = doc_bow.sort_values(ascending=False).head(n_terms)
    return top_terms

bow_df, vectorizer = calculate_bag_of_words(df)

top_terms = get_top_terms(bow_df, n_terms=10)
print("\nTop 10 terms across all documents:")
print(top_terms)

print("\nTop 5 terms in first document:")
print(get_document_top_terms(bow_df, 0))

bow_df.to_excel('bag_of_words_matrix.xlsx')

def create_summary_df(df, bow_df, n_terms=5):
    summaries = []
    for idx in range(len(df)):
        top_terms = get_document_top_terms(bow_df, idx, n_terms)
        summaries.append({
            'original_text': df['full_text'].iloc[idx],
            'top_terms': ', '.join(f"{term} ({count})" for term, count in top_terms.items())
        })
    return pd.DataFrame(summaries)

summary_df = create_summary_df(df, bow_df)
summary_df.to_excel('bag_of_words_summary.xlsx', index=False)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import ast

def prepare_text_data(df, token_column='filtered_tokens'):
    def safe_join(tokens):
        if isinstance(tokens, str):
            try:
                tokens = ast.literal_eval(tokens)
            except (ValueError, SyntaxError):
                tokens = tokens.split()

        if isinstance(tokens, list):
            tokens = [str(token).strip() for token in tokens if token]
            return ' '.join(tokens) if tokens else ''

        return ''

    processed_texts = df[token_column].apply(safe_join)

    if processed_texts.str.strip().empty:
        raise ValueError("No valid text data after processing. Check token column contents.")

    return processed_texts

def train_evaluate_nb(X_train, X_test, y_train, y_test, vectorizer_type="tfidf"):
    if vectorizer_type == "tfidf":
        vectorizer = TfidfVectorizer(
            token_pattern=r'\b\w+\b',
            min_df=1,
            stop_words=None
        )
    else:
        vectorizer = CountVectorizer(
            token_pattern=r'\b\w+\b',
            min_df=1,
            stop_words=None
        )

    X_train_transformed = vectorizer.fit_transform(X_train)
    X_test_transformed = vectorizer.transform(X_test)

    nb_model = MultinomialNB()
    nb_model.fit(X_train_transformed, y_train)

    y_pred = nb_model.predict(X_test_transformed)

    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

    try:
        feature_importance = pd.DataFrame({
            'feature': vectorizer.get_feature_names_out(),
            'importance': np.exp(nb_model.feature_log_prob_[1] - nb_model.feature_log_prob_[0])
        })
        feature_importance = feature_importance.sort_values('importance', ascending=False).head(10)
    except Exception as e:
        print(f"Error computing feature importance: {e}")
        feature_importance = pd.DataFrame()

    return {
        'model': nb_model,
        'vectorizer': vectorizer,
        'accuracy': accuracy,
        'confusion_matrix': conf_matrix,
        'classification_report': class_report,
        'feature_importance': feature_importance
    }

def inspect_data(df):
    print("DataFrame Info:")
    print(df.info())

    print("\nSample of 'filtered_tokens' column:")
    print(df['filtered_tokens'].head())

    print("\nTokens length:")
    print(df['filtered_tokens'].apply(lambda x: len(x) if isinstance(x, list) else 0).describe())

def main():

    df = pd.read_excel('preprocessing.xlsx')

    inspect_data(df)

    X = prepare_text_data(df)
    y = df['Fix Label']
    print("\nPrepared Text Samples:")
    print(X.head())

    # Split 80:20
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    results_tfidf = train_evaluate_nb(X_train, X_test, y_train, y_test, "tfidf")
    results_bow = train_evaluate_nb(X_train, X_test, y_train, y_test, "bow")

    print("\nResults for TF-IDF based Naive Bayes:")
    print(f"Accuracy: {results_tfidf['accuracy']:.4f}")
    print("\nClassification Report:")
    print(results_tfidf['classification_report'])
    print("\nTop 10 Important Features (TF-IDF):")
    print(results_tfidf['feature_importance'])

    print("\nResults for Bag of Words based Naive Bayes:")
    print(f"Accuracy: {results_bow['accuracy']:.4f}")
    print("\nClassification Report:")
    print(results_bow['classification_report'])
    print("\nTop 10 Important Features (BoW):")
    print(results_bow['feature_importance'])

if __name__ == '__main__':
    main()

from imblearn.over_sampling import SMOTE
from collections import Counter
import numpy as np

df['text_for_tfidf'] = df['filtered_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

tfidf_vectorizer = TfidfVectorizer(max_features=1000)
X = tfidf_vectorizer.fit_transform(df['text_for_tfidf'])
y = df['Fix Label']

print("Original class distribution:")
print(Counter(y))

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("\nBalanced class distribution:")
print(Counter(y_resampled))

X_resampled_df = pd.DataFrame(
    X_resampled.toarray(),
    columns=tfidf_vectorizer.get_feature_names_out()
)

balanced_df = pd.DataFrame({
    'text': list(df['text_for_tfidf']) + [f"synthetic_sample_{i}" for i in range(len(y_resampled) - len(df))],
    'label': y_resampled,
    'is_synthetic': [0] * len(df) + [1] * (len(y_resampled) - len(df))
})

print("\nOriginal dataset shape:", X.shape)
print("Resampled dataset shape:", X_resampled.shape)

balanced_df.to_excel('balanced_dataset.xlsx', index=False)

pip install pandas numpy scikit-learn seaborn matplotlib

import pandas as pd
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

print("Loading data...")
df = pd.read_excel('preprocessing.xlsx')
df['processed_text'] = df['filtered_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

X_train, X_test, y_train, y_test = train_test_split(
    df['processed_text'],
    df['Fix Label'],
    test_size=0.1,
    random_state=42,
    stratify=df['Fix Label']
)

params = {
    'max_features': [2000, 3000, 4000],
    'ngram_range': [(1,1), (1,2), (1,3)],
    'min_df': [1, 2, 3],
    'max_df': [0.9, 0.95, 1.0],
    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],
    'fit_prior': [True, False]
}

def find_best_params_separate(X_train, y_train, X_test, y_test, feature_type='bow'):
    best_score = 0
    best_params = None
    best_vectorizer = None
    best_model = None
    all_results = []

    total_combinations = (len(params['max_features']) *
                         len(params['ngram_range']) *
                         len(params['min_df']) *
                         len(params['max_df']) *
                         len(params['alpha']) *
                         len(params['fit_prior']))

    current = 0

    for max_feat in params['max_features']:
        for ngram in params['ngram_range']:
            for min_df in params['min_df']:
                for max_df in params['max_df']:
                    if feature_type == 'bow':
                        vectorizer = CountVectorizer(
                            max_features=max_feat,
                            ngram_range=ngram,
                            min_df=min_df,
                            max_df=max_df
                        )
                    else:
                        vectorizer = TfidfVectorizer(
                            max_features=max_feat,
                            ngram_range=ngram,
                            min_df=min_df,
                            max_df=max_df
                        )

                    X_train_transformed = vectorizer.fit_transform(X_train)

                    smote = SMOTE(random_state=42)
                    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_transformed, y_train)

                    for alpha in params['alpha']:
                        for fit_prior in params['fit_prior']:
                            current += 1
                            print(f"\rProgress {feature_type.upper()}: {current}/{total_combinations}", end="")

                            nb = MultinomialNB(alpha=alpha, fit_prior=fit_prior)
                            nb.fit(X_train_balanced, y_train_balanced)

                            X_test_transformed = vectorizer.transform(X_test)

                            score = nb.score(X_test_transformed, y_test)

                            result = {
                                'feature_type': feature_type,
                                'max_features': max_feat,
                                'ngram_range': str(ngram),
                                'min_df': min_df,
                                'max_df': max_df,
                                'alpha': alpha,
                                'fit_prior': fit_prior,
                                'score': score
                            }
                            all_results.append(result)

                            if score > best_score:
                                best_score = score
                                best_params = result
                                best_vectorizer = vectorizer
                                best_model = nb

    return best_params, best_score, best_vectorizer, best_model, all_results

print("\nMencari parameter terbaik untuk BOW...")
best_params_bow, best_score_bow, best_vectorizer_bow, best_model_bow, all_results_bow = find_best_params_separate(
    X_train, y_train, X_test, y_test, 'bow'
)

print("\nMencari parameter terbaik untuk TF-IDF...")
best_params_tfidf, best_score_tfidf, best_vectorizer_tfidf, best_model_tfidf, all_results_tfidf = find_best_params_separate(
    X_train, y_train, X_test, y_test, 'tfidf'
)

print("\n\nHasil Evaluasi BOW:")
print("Parameter terbaik BOW:", best_params_bow)
print("Best score BOW:", best_score_bow)

print("\nHasil Evaluasi TF-IDF:")
print("Parameter terbaik TF-IDF:", best_params_tfidf)
print("Best score TF-IDF:", best_score_tfidf)

def evaluate_model(model, vectorizer, X_test, y_test, feature_type):
    X_test_transformed = vectorizer.transform(X_test)
    y_pred = model.predict(X_test_transformed)

    print(f"\nHasil evaluasi model {feature_type.upper()}:")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {feature_type.upper()}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    return y_pred

y_pred_bow = evaluate_model(best_model_bow, best_vectorizer_bow, X_test, y_test, 'BOW')
y_pred_tfidf = evaluate_model(best_model_tfidf, best_vectorizer_tfidf, X_test, y_test, 'TF-IDF')

with pd.ExcelWriter('separate_features_nb_results.xlsx') as writer:

    pd.DataFrame(all_results_bow).to_excel(writer, sheet_name='BOW_All_Results', index=False)
    pd.DataFrame(all_results_tfidf).to_excel(writer, sheet_name='TFIDF_All_Results', index=False)

    pd.DataFrame([best_params_bow]).to_excel(writer, sheet_name='BOW_Best_Parameters', index=False)
    pd.DataFrame([best_params_tfidf]).to_excel(writer, sheet_name='TFIDF_Best_Parameters', index=False)

    bow_metrics = {
        'Feature_Type': ['BOW'],
        'Accuracy': [accuracy_score(y_test, y_pred_bow)],
        'Classification_Report': [classification_report(y_test, y_pred_bow)]
    }
    tfidf_metrics = {
        'Feature_Type': ['TF-IDF'],
        'Accuracy': [accuracy_score(y_test, y_pred_tfidf)],
        'Classification_Report': [classification_report(y_test, y_pred_tfidf)]
    }
    pd.concat([
        pd.DataFrame(bow_metrics),
        pd.DataFrame(tfidf_metrics)
    ]).to_excel(writer, sheet_name='Metrics', index=False)

def get_feature_importance(model, vectorizer, feature_type):
    feature_names = vectorizer.get_feature_names_out()
    feature_importance = []

    for i, class_label in enumerate(model.classes_):
        importance = model.feature_log_prob_[i]
        top_indices = np.argsort(importance)[-20:]

        features = {
            'feature': [feature_names[j] for j in top_indices],
            'importance': importance[top_indices],
            'feature_type': [feature_type] * 20
        }
        feature_importance.append({
            'class': class_label,
            'features': pd.DataFrame(features)
        })

    return feature_importance

bow_importance = get_feature_importance(best_model_bow, best_vectorizer_bow, "BOW")
tfidf_importance = get_feature_importance(best_model_tfidf, best_vectorizer_tfidf, "TF-IDF")

with pd.ExcelWriter('separate_feature_importance.xlsx') as writer:
    for class_data in bow_importance:
        sheet_name = f"Class_{class_data['class']}_BOW"[:31]
        class_data['features'].to_excel(writer, sheet_name=sheet_name, index=False)

    for class_data in tfidf_importance:
        sheet_name = f"Class_{class_data['class']}_TFIDF"[:31]
        class_data['features'].to_excel(writer, sheet_name=sheet_name, index=False)

print("\nPerbandingan Performa Model:")
print(f"BOW Accuracy: {accuracy_score(y_test, y_pred_bow):.4f}")
print(f"TF-IDF Accuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}")